import requests
import re
import os
import time
from bs4 import BeautifulSoup
from collections import defaultdict

IPINFO_TOKEN = os.environ.get("IPINFO_TOKEN")
if not IPINFO_TOKEN:
    raise EnvironmentError("IPINFO_TOKEN is not set in environment variables.")

SOURCE_URL = "https://ip.164746.xyz"
API_URL = f"https://ipinfo.io/{{ip}}?token={IPINFO_TOKEN}"

COUNTRY_MAP = {
    "HK": ("ğŸ‡­ğŸ‡°", "é¦™æ¸¯"),
    "CN": ("ğŸ‡¨ğŸ‡³", "ä¸­å›½"),
    "US": ("ğŸ‡ºğŸ‡¸", "ç¾å›½"),
    "JP": ("ğŸ‡¯ğŸ‡µ", "æ—¥æœ¬"),
    "SG": ("ğŸ‡¸ğŸ‡¬", "æ–°åŠ å¡"),
    "KR": ("ğŸ‡°ğŸ‡·", "éŸ©å›½"),
    "TW": ("ğŸ‡¹ğŸ‡¼", "å°æ¹¾"),
    "AU": ("ğŸ‡¦ğŸ‡º", "æ¾³å¤§åˆ©äºš"),
    "DE": ("ğŸ‡©ğŸ‡ª", "å¾·å›½"),
    "PH": ("ğŸ‡µğŸ‡­", "è²å¾‹å®¾"),
    "GB": ("ğŸ‡¬ğŸ‡§", "è‹±å›½"),
    "FR": ("ğŸ‡«ğŸ‡·", "æ³•å›½"),
    "IN": ("ğŸ‡®ğŸ‡³", "å°åº¦"),
    "TH": ("ğŸ‡¹ğŸ‡­", "æ³°å›½"),
    "ZZ": ("ğŸ³ï¸", "æœªçŸ¥")
}

country_data = defaultdict(list)

def normalize_ip(ip_line):
    """ä¿®æ”¹IPæ¸…æ´—é€»è¾‘ï¼Œå…ˆç§»é™¤å¼€å¤´çš„äº”è§’æ˜Ÿå’Œç©ºæ ¼"""
    # ç§»é™¤å¼€å¤´çš„äº”è§’æ˜Ÿ(â˜…)å’Œå¯èƒ½çš„ç©ºæ ¼
    ip_line = re.sub(r'^â˜…\s*', '', ip_line)  # æ–°å¢è¿™ä¸€è¡Œå¤„ç†äº”è§’æ˜Ÿæ ‡è®°
    ip_line = ip_line.strip().split("#")[0].split("//")[-1]
    if not ip_line:
        return None
    match = re.match(r"^(\d{1,3}(?:\.\d{1,3}){3})(?::(\d{1,5}))?$", ip_line)
    if not match:
        return None
    ip, port = match.groups()
    return ip if not port else f"{ip}:{port}"

def get_ip_info(ip):
    base_ip = ip.split(":")[0]
    try:
        res = requests.get(API_URL.format(ip=base_ip), timeout=5)
        if res.status_code != 200:
            return None
        data = res.json()
        code = data.get("country", "ZZ")
        emoji, name = COUNTRY_MAP.get(code, ("ğŸ³ï¸", "æœªçŸ¥"))
        return {
            "ip": ip,
            "country_code": code,
            "country_name": name,
            "emoji": emoji
        }
    except Exception as e:
        print(f"è·å–IPä¿¡æ¯å¤±è´¥ {ip}: {e}")
        return None

def extract_ips_from_table():
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        }
        
        resp = requests.get(SOURCE_URL, headers=headers, timeout=10)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.text, "html.parser")
        table = soup.find("table")
        if not table:
            print("æœªæ‰¾åˆ°è¡¨æ ¼å…ƒç´ ")
            return []
            
        ip_list = []
        rows = table.find_all("tr")
        
        for row in rows[1:]:
            cells = row.find_all("td")
            if cells:
                ip_candidate = cells[0].text.strip()
                normalized_ip = normalize_ip(ip_candidate)
                if normalized_ip:
                    ip_list.append(normalized_ip)
        
        print(f"ä»è¡¨æ ¼ä¸­æå–åˆ° {len(ip_list)} ä¸ªæœ‰æ•ˆIPåœ°å€")
        return ip_list
        
    except Exception as e:
        print(f"æå–IPæ—¶å‡ºé”™: {e}")
        return []

def main():
    raw_ips = extract_ips_from_table()
    
    if not raw_ips:
        print("æ²¡æœ‰æå–åˆ°ä»»ä½•IPåœ°å€")
        with open("top10.txt", "w", encoding="utf-8") as f:
            f.write("")
        return

    for ip in raw_ips:
        info = get_ip_info(ip)
        if not info:
            print(f"æ— æ³•è·å–ä¿¡æ¯: {ip}")
            continue
        key = info["country_code"]
        if len(country_data[key]) < 10:
            label = f'{info["ip"]}#{info["emoji"]}{key}-{info["country_name"]}-å¤‡ç”¨'
            country_data[key].append(label)
        time.sleep(1)

    all_lines = []
    for lines in country_data.values():
        all_lines.extend(lines)

    with open("top10.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(all_lines) + "\n")
    print(f"å·²å†™å…¥ {len(all_lines)} æ¡æ•°æ®åˆ° top10.txt")

if __name__ == "__main__":
    main()
